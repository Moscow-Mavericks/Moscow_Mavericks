{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizaje supervisado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')\n",
    "import random\n",
    "random.seed(1)\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datos\n",
    "\n",
    "Vamos a realizar el proceso con dos selección de variables distintas para ver con cual obtenemos mejores resultados:\n",
    "\n",
    "* Selección 1: Las 25 variables más importantes obtenidas en el apartado de \"Selección de Variables\" del primer trimestre.\n",
    "\n",
    "* Selección 2: Formada por las variables arrival_month, deposit_type_Non Refund, is_repeated_guest, reserverd/assigned, hotel_Resort Hotel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>is_canceled</th>\n",
       "      <th>lead_time</th>\n",
       "      <th>arrival_date_year</th>\n",
       "      <th>arrival_date_week_number</th>\n",
       "      <th>arrival_date_day_of_month</th>\n",
       "      <th>stays_in_weekend_nights</th>\n",
       "      <th>stays_in_week_nights</th>\n",
       "      <th>adults</th>\n",
       "      <th>children</th>\n",
       "      <th>...</th>\n",
       "      <th>hotel_Resort Hotel</th>\n",
       "      <th>meal_FB</th>\n",
       "      <th>meal_HB</th>\n",
       "      <th>meal_SC</th>\n",
       "      <th>meal_Undefined</th>\n",
       "      <th>deposit_type_Non Refund</th>\n",
       "      <th>deposit_type_Refundable</th>\n",
       "      <th>reserverd/assigned</th>\n",
       "      <th>customer_type_Group</th>\n",
       "      <th>customer_type_Transient-Party</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.077658</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.389493</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.755577</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.077658</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.389493</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  is_canceled  lead_time  arrival_date_year  \\\n",
       "0           0          0.0  13.077658             2015.0   \n",
       "1           1          1.0   7.389493             2015.0   \n",
       "2           2          0.0   7.755577             2015.0   \n",
       "3           3          0.0  13.077658             2015.0   \n",
       "4           4          1.0   7.389493             2015.0   \n",
       "\n",
       "   arrival_date_week_number  arrival_date_day_of_month  \\\n",
       "0                      27.0                        1.0   \n",
       "1                      27.0                        1.0   \n",
       "2                      27.0                        1.0   \n",
       "3                      27.0                        1.0   \n",
       "4                      27.0                        1.0   \n",
       "\n",
       "   stays_in_weekend_nights  stays_in_week_nights  adults  children  ...  \\\n",
       "0                      2.0                   5.0     2.0       0.0  ...   \n",
       "1                      0.0                   2.0     2.0       0.0  ...   \n",
       "2                      0.0                   2.0     1.0       0.0  ...   \n",
       "3                      0.0                   4.0     2.0       0.0  ...   \n",
       "4                      0.0                   2.0     2.0       0.0  ...   \n",
       "\n",
       "   hotel_Resort Hotel  meal_FB  meal_HB  meal_SC  meal_Undefined  \\\n",
       "0                 1.0      0.0      1.0      0.0             0.0   \n",
       "1                 0.0      0.0      1.0      0.0             0.0   \n",
       "2                 0.0      0.0      1.0      0.0             0.0   \n",
       "3                 1.0      0.0      0.0      0.0             0.0   \n",
       "4                 0.0      0.0      1.0      0.0             0.0   \n",
       "\n",
       "   deposit_type_Non Refund  deposit_type_Refundable  reserverd/assigned  \\\n",
       "0                      0.0                      0.0                 0.0   \n",
       "1                      0.0                      0.0                 1.0   \n",
       "2                      0.0                      0.0                 1.0   \n",
       "3                      0.0                      0.0                 0.0   \n",
       "4                      0.0                      0.0                 1.0   \n",
       "\n",
       "   customer_type_Group  customer_type_Transient-Party  \n",
       "0                  0.0                            0.0  \n",
       "1                  0.0                            1.0  \n",
       "2                  0.0                            1.0  \n",
       "3                  0.0                            1.0  \n",
       "4                  0.0                            1.0  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selección de datos 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = data.iloc[:,2:30].values\n",
    "Y1 = data.iloc[:,1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (20100, 28) (20100,)\n",
      "Test (9900, 28) (9900,)\n"
     ]
    }
   ],
   "source": [
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X1, Y1, test_size=0.33, random_state=1)\n",
    "print('Train', X_train1.shape, y_train1.shape)\n",
    "print('Test', X_test1.shape, y_test1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selección de datos 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vars selected\n",
    "vars_s=['arrival_month','deposit_type_Non Refund','is_repeated_guest','reserverd/assigned','hotel_Resort Hotel']\n",
    "\n",
    "Y2 = data.is_canceled\n",
    "X2 = data[vars_s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, Y2, test_size=0.33, random_state=1)\n",
    "print('Train', X_train2.shape, y_train2.shape)\n",
    "print('Test', X_test2.shape, y_test2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresion Logística\n",
    "\n",
    "Mediante la RL se pretende modelar la probabilidad de que ocurra el hecho en cuestión como función de ciertas variables que se presumen relevantes o influyentes. Por lo tanto, la RL consiste en obtener una función logística de las variables independientes que permita clasificar a los individuos en una de las dos subpoblaciones o grupos establecidos por los dos va­lores de la variable dependiente.\n",
    "\n",
    "La función logística es aquella que halla, para cada individuo según los valores de una serie de variables (Xi), la probabilidad (p) de que presente el efecto estudiado. Una transformación logarítmica de dicha ecuación, a la que se le llama logit. De aquí surge la ecuación de la regresión logística, que es parecida a la ecuación de la regresión lineal múltiple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(X_train, y_train,X_test, y_test, model):\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=3, scoring=\"accuracy\")\n",
    "    diff = scores.mean() - model.score(X_test, y_test)\n",
    "    SD = diff / scores.std()\n",
    "    \n",
    "    print(f\"Training Score:{model.score(X_train, y_train)}\")\n",
    "    print(f\"Cross V Score: {scores.mean()} +/- {scores.std()}\")\n",
    "    print(f\"Testing Score: {model.score(X_test, y_test)}\")\n",
    "    print(f\"Cross & Test Diff: {diff}\")\n",
    "    print(f\"Standard Deviations Away: {SD}\")\n",
    "    print(confusion_matrix(y_test, preds))\n",
    "   \n",
    "    print(classification_report(y_test, preds))\n",
    "    \n",
    "    logit_roc_auc = roc_auc_score(y_test, preds)\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, model.predict_proba(X_test)[:,1])\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balanceado\n",
    "\n",
    "Para el balanceado de los datos utilizaremos la tecnica SMOTE (Synthetic Minority Oversampling Technique).\n",
    "\n",
    "SMOTE es una técnica estadística de sobremuestreo de minorías sintéticas para aumentar el número de casos de un conjunto de datos de forma equilibrada. El módulo funciona cuando genera nuevas instancias a partir de casos minoritarios existentes que se proporcionan como entrada. SMOTE no cambia el número de casos de mayoría.\n",
    "\n",
    "Las instancias nuevas no son meras copias de los casos minoritarios existentes. En su lugar, el algoritmo toma muestras del espacio de características de cada clase de destino y de sus vecinos más próximos. Luego, el algoritmo genera nuevos ejemplos que combinan las características del caso que nos ocupa con características de sus vecinos. Este enfoque aumenta las características disponibles para cada clase y hace que las muestras sean más generales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os = SMOTE(random_state=0)\n",
    "os_data_X,os_data_y=os.fit_sample(X_train1, y_train1)\n",
    "os_data_2X,os_data_2y=os.fit_sample(X_train2, y_train2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selección de datos 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_log = LogisticRegression(random_state=0, solver='newton-cg', max_iter=1000)\n",
    "evaluate_model(os_data_X, os_data_y, X_test1, y_test1, reg_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selección de datos 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_log = LogisticRegression(random_state=0, solver='newton-cg', max_iter=1000)\n",
    "evaluate_model(os_data_2X, os_data_2y, X_test2, y_test2, reg_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN\n",
    "\n",
    "El algoritmo KNN (K Nearest Neighbours) clasifica cada dato nuevo en el grupo que corresponda, según tenga k vecinos más cerca de un grupo o de otro. Es decir, calcula la distancia del elemento nuevo a cada uno de los existentes, y ordena dichas distancias de menor a mayor para ir seleccionando el grupo al que pertenecer. Este grupo será, por tanto, el de mayor frecuencia con menores distancias.\n",
    "\n",
    "En contraste con otros algoritmos de aprendizaje supervisado, K-NN no genera un modelo fruto del aprendizaje con datos de entrenamiento, sino que el aprendizaje sucede en el mismo momento en el que se prueban los datos de test. A este tipo de algoritmos se les llama lazy learning methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selección de datos 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train1\n",
    "y_train = y_train1\n",
    "\n",
    "X_test = X_test1\n",
    "y_test = y_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_stand = scaler.transform(X_train)\n",
    "X_test_stand = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero estandarizamos los valores.\n",
    "\n",
    "Ahora vamos a buscar cual es el valor de K más óptimo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = []\n",
    "\n",
    "for i in range(1, 30):\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn.fit(X_train_stand, y_train)\n",
    "    pred_i = knn.predict(X_test_stand)\n",
    "    error.append(np.mean(pred_i != y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(1, 30), error, color='red', linestyle='dashed', marker='o',\n",
    "         markerfacecolor='blue', markersize=10)\n",
    "plt.title('Error Rate K Value')\n",
    "plt.xlabel('K Value')\n",
    "plt.ylabel('Mean Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos quedamos con K = 3 para entrenar el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = KNeighborsClassifier(n_neighbors=3)\n",
    "classifier.fit(X_train_stand, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test_stand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sacamos el informe de clasificación que hemos obtenido:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De esta forma obtenemos una precisión de 89% en ambas clases con un recall de 94% y 80% respectivamente.\n",
    "Lo que significa que encontramos el 80% de casos en los que se cancela la reserva, y predecimos correctamente el 90% de esos.\n",
    "\n",
    "(El recall nos indica el porcentaje encontrado de esa clase con respecto a todos los elementos de esa misma clase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selección de datos 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train2\n",
    "y_train = y_train2\n",
    "\n",
    "X_test = X_test2\n",
    "y_test = y_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_stand = scaler.transform(X_train)\n",
    "X_test_stand = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero estandarizamos los valores.\n",
    "\n",
    "Ahora vamos a buscar cual es el valor de K más óptimo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = []\n",
    "\n",
    "# Calculating error for K values between 1 and 40\n",
    "for i in range(1, 30):\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn.fit(X_train_stand, y_train)\n",
    "    pred_i = knn.predict(X_test_stand)\n",
    "    error.append(np.mean(pred_i != y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(1, 30), error, color='red', linestyle='dashed', marker='o',\n",
    "         markerfacecolor='blue', markersize=10)\n",
    "plt.title('Error Rate K Value')\n",
    "plt.xlabel('K Value')\n",
    "plt.ylabel('Mean Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que a partir de K = 18 el valor no se mejora, por lo que nos quedamos con ese valor para K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = KNeighborsClassifier(n_neighbors=16)\n",
    "classifier.fit(X_train_stand, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test_stand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sacamos el informe de clasificación que hemos obtenido:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esta selección de variables obtenemos más precisión en cada una de las clases, pero un recall mucho más bajo, por lo que no está encontrado todos los elementos de la clase correspondiente.\n",
    "\n",
    "Concluimos por lo tanto que se obtienen mejores resultados con la primera selección de datos, donde podemos decir sacamos un buen resultado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM\n",
    "\n",
    "El método de clasificación-regresión Máquinas de Vector Soporte (Vector Support Machines, SVMs) fue desarrollado en la década de los 90, dentro de campo de la ciencia computacional. Si bien originariamente se desarrolló como un método de clasificación binaria, su aplicación se ha extendido a problemas de clasificación múltiple y regresión. SVMs ha resultado ser uno de los mejores clasificadores para un amplio abanico de situaciones, por lo que se considera uno de los referentes dentro del ámbito de aprendizaje estadístico y machine learning.\n",
    "\n",
    "Las Máquinas de Vector Soporte se fundamentan en el Maximal Margin Classifier, que a su vez, se basa en el concepto de hiperplano.\n",
    "\n",
    "Las ventajas de las máquinas de vectores de soporte son:\n",
    "* Eficaces en espacios de gran dimensión. Siguen siendo eficaces en los casos en que el número de dimensiones es mayor que el número de muestras.\n",
    "* Utiliza un subconjunto de puntos de entrenamiento en la función de decisión (llamados vectores de soporte), por lo que también es eficiente en la memoria.\n",
    "* Versátil: se pueden especificar diferentes funciones kernel para la función de decisión. Se proporcionan kernels comunes en sklearn, pero también es posible especificar kernels personalizados.\n",
    "\n",
    "Las desventajas de las máquinas de vectores de soporte incluyen:\n",
    "* Si el número de características es mucho mayor que el número de muestras, evitar el ajuste excesivo es crucial al elegir las funciones del Kernel y el término de regularización.\n",
    "* Las SVM no proporcionan directamente estimaciones de probabilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch_SVM(X_train,y_train,X_test,y_test,kernel='linear'):\n",
    "    svc = SVC(kernel=kernel)\n",
    "    svc.fit(X_train, y_train)\n",
    "    y_pred = svc.predict(X_test)\n",
    "    confusion_matrix(y_test, y_pred)\n",
    "    print('---Matriz de confusión---')\n",
    "    print()\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "    print()\n",
    "    print('---Report---')\n",
    "    print(classification_report(y_test1,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selección de datos 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel lineal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "launch_SVM(X_train1,y_train1,X_test1,y_test1,kernel='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel Gaussiano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "launch_SVM(X_train1,y_train1,X_test1,y_test1,kernel='rbf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel Sigmoide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "launch_SVM(X_train1,y_train1,X_test1,y_test1,kernel='sigmoid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selección de datos 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel lineal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "launch_SVM(X_train2,y_train2,X_test2,y_test2,kernel='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel Gaussiano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "launch_SVM(X_train2,y_train2,X_test2,y_test2,kernel='rbf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel Sigmoide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "launch_SVM(X_train1,y_train1,X_test1,y_test1,kernel='sigmoid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar el kernel que mejor resultados nos reporta es el lineal. Además haciendo uso del conjunto de features 1 obtenemos una precisión muy alta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Ajuste de Hiperparametros\n",
    "\n",
    "Los hiperparámetros son parámetros que no se aprenden directamente en los estimadores. Los ejemplos típicos para el caso de las SVM incluyen C, kernel y gamma, pero en realidad, Cualquier parámetro proporcionado al construir un estimador puede ser optimizado de esta manera.\n",
    "\n",
    "Un ajuste consiste primordialmente en:\n",
    "\n",
    "* Un estimador, en nuestro caso SVM\n",
    "\n",
    "* Un espacio de parámetros\n",
    "\n",
    "* Un método de búsqueda o muestreo de candidatos\n",
    "\n",
    "* Un esquema de validación cruzada; y\n",
    "\n",
    "* Una función de puntuación.\n",
    "\n",
    "En scikit-learn se proporcionan dos enfoques genéricos para la búsqueda de parámetros:\n",
    "\n",
    "- GridSearchCV considera exhaustivamente todas las combinaciones de parámetros\n",
    "\n",
    "- RandomizedSearchCV puede muestrear un número determinado de candidatos de un espacio de parámetros con una distribución especificada.\n",
    "\n",
    "Haciendo uso de estas herramientas, repetiremos el proceso anterior, pero sumarizando mucho mejor los resultados, y conseguiremos un mayor numero de combinaciones.\n",
    "\n",
    "Compararemos el rendimiento de los las SVM que varían en su parámetro de kernel, para decidir qué elección de este hiperparámetro predice mejor nuestros datos. Evaluaremos el rendimiento de los modelos utilizando RepeatedStratifiedKFold, repitiendo 10 veces una validación cruzada y 10 veces utilizando una aleatorización diferente de los datos. \n",
    "\n",
    "El rendimiento se evaluará utilizando roc_auc_score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {'C': [1, 10],'gamma': [0.001, 0.0001],'kernel': ['linear']}\n",
    "    #{'C': [1, 10],'gamma': [0.001, 0.0001],'kernel': ['poly'], 'degree': [2, 3]},\n",
    "    #{'C': [1, 10],'gamma': [0.001, 0.0001],'kernel': ['rbf']},\n",
    "    #{'C': [1, 10],'gamma': [0.001, 0.0001],'kernel': ['sigmoid']}\n",
    "]\n",
    "\n",
    "svc = SVC(random_state=0)\n",
    "\n",
    "cv = RepeatedStratifiedKFold(\n",
    "    n_splits=5, n_repeats=5, random_state=0\n",
    ")\n",
    "\n",
    "search = GridSearchCV(\n",
    "    estimator=svc, param_grid=param_grid,\n",
    "    scoring='roc_auc', cv=cv\n",
    ")\n",
    "search.fit(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraemos los resultados a un dataframe para poder comparar el rendimiento de los modelos ajustados con los diferentes parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(search.cv_results_)\n",
    "results_df = results_df.sort_values(by=['rank_test_score'])\n",
    "results_df = (\n",
    "    results_df\n",
    "    .set_index(results_df[\"params\"].apply(\n",
    "        lambda x: \"_\".join(str(val) for val in x.values()))\n",
    "    )\n",
    "    .rename_axis('kernel')\n",
    ")\n",
    "results_df[\n",
    "    ['params', 'rank_test_score', 'mean_test_score', 'std_test_score']\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero no sabemos si estos modelos son significativos. Para evaluar esto, necesitamos realizar una prueba estadística. En concreto, para contrastar el rendimiento de dos modelos debemos comparar estadísticamente sus puntuaciones AUC.\n",
    "\n",
    "Sin embargo, las puntuaciones de los modelos no son independientes: todos los modelos se evalúan en las mismas particiones, lo que aumenta la correlación entre el rendimiento de los modelos. Dado que algunas particiones de los datos pueden hacer que la distinción de las clases sea particularmente fácil o difícil de encontrar para todos los modelos, las puntuaciones de los modelos covarían.\n",
    "\n",
    "Trazando el rendimiento de todos los modelos en cada pliegue y calculando la correlación entre los modelos comprobaremos este efecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scores = results_df.filter(regex=r'split\\d*_test_score')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.lineplot(\n",
    "    data=model_scores.transpose().iloc[:30],\n",
    "    dashes=False, palette='Set1', marker='o', alpha=.5, ax=ax\n",
    ")\n",
    "ax.set_xlabel(\"CV test fold\", size=12, labelpad=10)\n",
    "ax.set_ylabel(\"Model AUC\", size=12)\n",
    "ax.tick_params(bottom=True, labelbottom=False)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Correlation of models:\\n {model_scores.transpose().corr()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar que el rendimiento de los modelos depende en gran medida del pliegue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
